# üìÑ README: Proyecto TFM - Procesamiento de Datos en Tiempo Real y An√°lisis de Churn

## üöÄ Visi√≥n General

Este repositorio contiene el c√≥digo y la infraestructura necesarios para un proyecto de Trabajo de Fin de M√°ster (TFM) centrado en el **procesamiento de datos semi-estructurados en tiempo real** (utilizando Apache Kafka y Apache Spark Streaming) y el **an√°lisis del churn** (abandono de clientes) en el sector de las telecomunicaciones.

La infraestructura se orquesta mediante **Docker Compose** para asegurar un entorno de desarrollo reproducible y coherente.

## üèóÔ∏è Estructura del Proyecto

El directorio ra√≠z (`TFM`) organiza el proyecto en los siguientes componentes clave:

| Directorio/Archivo | Descripci√≥n |
|-------------------|-------------|
| `database/` | Contiene los scripts de inicializaci√≥n de la base de datos PostgreSQL. Incluye la creaci√≥n de la base de datos, las tablas, los √≠ndices (para optimizar el rendimiento de las consultas y la velocidad de b√∫squeda) y una vista predefinida para facilitar la consulta r√°pida de datos completos de un cliente. |
| `scripts/` | Almacena los principales scripts de Python para la l√≥gica de negocio del proyecto. Estos scripts son montados como vol√∫menes en los contenedores de Spark y Spark Notebook. |
| `scripts/Kafka/` | Contiene la l√≥gica para la ingesti√≥n y el manejo de datos con Apache Kafka. Hay un √∫nico script principal aqu√≠ para el rol de **Productor** que simula el env√≠o de datos de churn. |
| `scripts/spark/` | Contiene la l√≥gica para el procesamiento de streaming de datos con Apache Spark. Hay un √∫nico script principal aqu√≠ que act√∫a como **Consumidor** de Kafka y realiza el procesamiento. |
| `Health/` | Carpeta dedicada a los scripts o archivos para realizar chequeos de salud (Health Checks) de los servicios en los contenedores de Docker, asegurando que cada componente est√© operativo. |
| `Attachments/` | Carpeta para archivos de entrada semi-estructurados, como `telecom_churn_semi_structured.json`, que se utilizar√°n para la simulaci√≥n de datos o pruebas. |
| `.vscode/` | Configuraci√≥n espec√≠fica del editor VS Code (opcional). |
| `.env`/`.gitignore` | Archivos de configuraci√≥n de entorno y de exclusi√≥n de archivos para el control de versiones. |
| `venv/` | Entorno virtual de Python para gestionar las dependencias locales. |
| `requirements.txt` | Lista de dependencias de Python del proyecto. |
| `docker-compose.yml` | Archivo de configuraci√≥n que define y orquesta todos los servicios de la infraestructura Docker. |

## ‚öôÔ∏è Infraestructura con Docker Compose

La infraestructura del proyecto se levanta con `docker-compose.yml`, que define los siguientes servicios clave:

| Servicio | Imagen Base | Puerto | Descripci√≥n |
|----------|-------------|--------|-------------|
| `postgres-tfm` | `postgres:15` | 5433:5432 | Base de datos PostgreSQL para persistir los resultados del an√°lisis de churn. Se inicializa con los scripts de la carpeta `database/`. |
| `pgadmin-tfm` | `dpage/pgadmin4` | 8080:80 | Interfaz web PgAdmin para gestionar la base de datos PostgreSQL. |
| `zookeeper` | `confluentinc/cp-zookeeper` | 2181:2181 | Coordinador necesario para que Kafka funcione. |
| `kafka` | `confluentinc/cp-kafka` | 9092:9092 | Broker de mensajes Apache Kafka para la ingesta de datos en streaming. |
| `spark` | `apache/spark:3.5.0` | 7077, 8081:8080 | Master de Apache Spark para el procesamiento de datos en tiempo real (Spark Streaming). |
| `spark-notebook` | `jupyter/pyspark-notebook` | 8888:8888 | Entorno Jupyter Notebook preconfigurado con PySpark, ideal para desarrollo, prototipado y an√°lisis exploratorio de datos. |

## üíª Puesta en Marcha del Proyecto

Para levantar la infraestructura completa, aseg√∫rate de tener **Docker** y **Docker Compose** instalados.

### 1. Inicializaci√≥n

Ejecuta el siguiente comando desde la ra√≠z del proyecto para construir y levantar todos los servicios definidos:

```bash
docker-compose up -d
```

### 2. Ejecuci√≥n de Tareas

Una vez que los contenedores est√©n operativos:

- **Productor Kafka**: El script principal dentro de `scripts/Kafka/` debe ejecutarse para simular la generaci√≥n y el env√≠o de los eventos de churn al topic de Kafka.

- **Procesador Spark**: El script principal dentro de `scripts/spark/` debe ejecutarse para consumir los datos de Kafka, aplicar la l√≥gica de procesamiento y anal√≠tica, y persistir los resultados en PostgreSQL.

### 3. Acceso a Interfaces

Puedes acceder a las siguientes interfaces web:

- **PgAdmin**: http://localhost:8080 (Consulta las credenciales en `docker-compose.yml`).
- **Spark Notebook (Jupyter)**: http://localhost:8888 (Utiliza el token definido en `docker-compose.yml`).

## üìå Dependencias y Configuraci√≥n

- El archivo `requirements.txt` detalla las librer√≠as de Python necesarias para la ejecuci√≥n local o dentro de los contenedores (p.ej., `pyspark`, `kafka-python`, `psycopg2`).
- Los contenedores de Spark y Spark Notebook tienen montado el directorio `scripts/` para poder acceder y ejecutar los scripts de procesamiento.